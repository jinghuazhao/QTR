## P values from linear and linear mixed models

As its name implies, P values are provided by the `lmerTest` package, which will be illustrated with
the documentation example here.
```{r message=FALSE}
require(lmerTest)
boxplot(Reaction~Days, data=sleepstudy, main="Reaction by Days",
        xlab="Days", ylab="Reaction", col="blue", border="black")
```

We see a trend of `Reaction` by `Days`, so it is reasonable to fit a linear regression to quantify the
relationship observed,
```{r}
l <- lm(Reaction ~ Days, sleepstudy)
s <- summary(l)
s
names(s)
```
showing significant association between `Reaction` and `Days`. We now turn to the following question:
how does the association alter after accounting for individual differences? The impact of `Subject` effect can
be revealed as follows,

```{r}
boxplot(Reaction~Subject, data=sleepstudy, main="Reaction by Subject",
        xlab="Subject", ylab="Reaction", col="orange", border="brown")
```

suggesting it is more approriate to fit a random effect model, in the sense that the `Subject` effect is
purely a random phenomenon in the association of interest:

```{r}
r <- lme4::lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
s <- summary(r)
s
names(s)
```
We see the same estimate of effect but a larger standard error for `Days` in the linear mixed model compared to
that in the linear regression model. We then use `lmer` from `lmerTest`.
```{r}
m <- lmerTest::lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
class(m)
s <-summary(m)
s
names(s)
with(s,coefficients)[2,5]
```
The P value for `Days` differs by orders of magnitude from that from a linear regression.
